{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Colorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook creates a model that is able to colorize images to a certain extent, which combines a Fast deep Convolutional Neural Network trained from scratch with high-level features extracted from the MobileNet pre-trained model. This encoder-decoder model can process images of any size and aspect ratio. The training of this model is done on 60K images of MS-COCO dataset. How this model performs in coloring images are also showing in result section.\n",
    "\n",
    "This notebook's work is inspired from https://github.com/titu1994/keras-mobile-colorizer which is also transfer to ipynb notebook too [[link]()]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "There is something uniquely and powerfully satisfying about the simple act of adding color to black and white imagery. Moreover this coloring of gray-scale images can have a big impact in a wide variety of domains, for instance, re-master of historical images, dormant memories or expressing artistic creativity and improvement of surveillance feeds.\n",
    "\n",
    "The information content of a gray-scale image is rather limited, thus adding the color components can provide more insights about its semantics. In the context of deep learning, models such as Inception [[ref]()], VGG [[ref]()] and others are usually trained using colored image datasets. When applying these networks on grayscale images, a prior colorization step can help improve the results. However, designing and implementing an effective and reliable system that automates this process still remains nowadays as a challenging task.\n",
    "\n",
    "In this regard, below is the proposed model that is able to colorize images to a certain extent, combining a DCNN [[ref]()] architecture which utilizes a U-Net inspired model conditioned on MobileNet class features to generate a mapping from Grayscale to Color image. This work is based on the https://github.com/titu1994/keras-mobile-colorizer and https://github.com/baldassarreFe/deep-koalarization [[research paper](https://arxiv.org/pdf/1712.03400.pdf)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My contribution are as follows: \n",
    "  \n",
    "1. Using fast algorithms for CNNs based on the minimal filtering algorithms pioneered by Winograd [[research paper](https://arxiv.org/abs/1509.09308)]\n",
    "2. Analysis and intuition behind a colorization architecture based on CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "Below are the few components that are used in the architecture of this model. Basic introduction of these components are given and more information you see the links provided in each section.\n",
    "\n",
    "\n",
    "### U-Net: Convolutional Networks\n",
    "\n",
    "The U-Net architecture is illustrated in below shown figure. It consists of a contracting path (left side) and an expansive path (right side). The architecture is that in the upsampling part, it has a large number of feature channels, which allow the network to propagate context information to higher resolution layers. As a consequence,\n",
    "the expansive path is more or less symmetric to the contracting path, and yields a u-shaped architecture.\n",
    "\n",
    "The contracting path follows the typical architecture of a convolutional network. It consists of the repeated application of two convolutions mostly 3x3 (unpadded convolutions), each followed by a rectified linear unit (ReLU) and a max pooling mostly 2x2 operation with stride 2 for downsampling. At each downsampling step we double the number of feature channels. Every step in the expansive path consists of an upsampling of the feature map followed by a convolution mostly 2x2 (“up-convolution”) that halves the number of feature channels, a concatenation with the correspondingly cropped feature map from the contracting path, and two convolutions mostly 2x2 , each followed by a ReLU. The cropping is necessary due to the loss of border pixels in every convolution. At the final layer a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. In total the below network has 23 convolutional layers.\n",
    "\n",
    "\n",
    "![U-Net Architecture](https://raw.githubusercontent.com/krypten/MobileDeepColorization/master/docs/images/u_net_architecture.png)\n",
    "\n",
    "\n",
    "\n",
    "### MobileNet  \n",
    "  \n",
    "MobileNets are an efficent class of convolutional neural network. The main difference between the MobileNet architecture and a “traditional” CNN’s is instead of a single 3x3 convolution layer followed by batch norm and ReLU, MobileNets split the convolution into a 3x3 depthwise convolution and a 1×1 convolution called a pointwise convolution.\n",
    "\n",
    "\n",
    "MobileNets introduce two simple global hyperparameters that efficiently trade off between latency and\n",
    "accuracy : width multiplier and resolution multiplier. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. The width multiplier allows us to thin the network, while the resolution multiplier changes the input dimensions of the image, reducing the internal representation at every layer.\n",
    "\n",
    "To learn more about how MobileNets work, read the [research paper](https://arxiv.org/pdf/1704.04861.pdf).\n",
    "\n",
    "![MobileNet convolution](https://raw.githubusercontent.com/krypten/MobileDeepColorization/master/docs/images/mobilenet_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "...\n",
    "\n",
    "\n",
    "![Deep Colorization Architecture](https://raw.githubusercontent.com/krypten/MobileDeepColorization/master/docs/images/deep_colorization_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'capt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e4ff76160ba8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{sys.executable} -m pip install tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcapt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Only display the errors and suppressed other output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'capt' is not defined"
     ]
    }
   ],
   "source": [
    "%%capture capt\n",
    "\n",
    "# Install pip packages in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install keras\n",
    "!{sys.executable} -m pip install scikit-image\n",
    "!{sys.executable} -m pip install tensorflow\n",
    "\n",
    "print(capt.stderr) # Only display the errors and suppressed other output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-730274e27ea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mweights_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'weights/mobilenet_model_improved.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "## Import required modules\n",
    "import keras\n",
    "from keras.layers import Conv2D, Input, Reshape, RepeatVector, concatenate, UpSampling2D, Flatten, Conv2DTranspose\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from keras.losses import mean_squared_error\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "\n",
    "weights_file_name = 'weights/mobilenet_model_improved.h5'\n",
    "\n",
    "# Fetch the util file and import it\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the Version of TensorFlow and Access to GPU¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "\n",
    "# Don't pre-allocate memory; allocate as-needed\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "# Create a session with the above options specified.\n",
    "K.tensorflow_backend.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the Data\n",
    "Run the following cell to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Data for use\n",
    "util.configure_tensforflow()\n",
    "\n",
    "# Load preprocessed data\n",
    "util.load_preprocessed_tfrecord_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters\n",
    "batch_size = 100\n",
    "epochs = 100\n",
    "image_size = 256\n",
    "# nb_train_images = 60000 # there are 82783 images in MS-COCO, set this to how many samples you want to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "# Load data\n",
    "(y_train, _), (y_test, _) = cifar10.load_data()\n",
    "x_train = np.expand_dims(rgb2gray(y_train), axis=3)\n",
    "x_test = np.expand_dims(rgb2gray(y_test), axis=3)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height, img_width = x_train.shape[1], x_train.shape[2]\n",
    "print(\"Image Height : {}, Weight : {}\".format(img_height, img_width))\n",
    "\n",
    "image_size = img_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics and Loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_weight = 1.0 #1e-3\n",
    "\n",
    "# set these to zeros to prevent learning\n",
    "perceptual_weight = 1. / (2. * 128. * 128.) # scaling factor\n",
    "attention_weight = 1.0 # 1.0\n",
    "\n",
    "\n",
    "# shows the minimum value of the AB channels\n",
    "def y_true_min(yt, yp):\n",
    "    return K.min(yt)\n",
    "\n",
    "\n",
    "# shows the maximum value of the RGB AB channels\n",
    "def y_true_max(yt, yp):\n",
    "    return K.max(yt)\n",
    "\n",
    "\n",
    "# shows the minimum value of the predicted AB channels\n",
    "def y_pred_min(yt, yp):\n",
    "    return K.min(yp)\n",
    "\n",
    "\n",
    "# shows the maximum value of the predicted AB channels\n",
    "def y_pred_max(yt, yp):\n",
    "    return K.max(yp)\n",
    "\n",
    "\n",
    "def gram_matrix(x):\n",
    "    assert K.ndim(x) == 4\n",
    "\n",
    "    with K.name_scope('gram_matrix'):\n",
    "        if K.image_data_format() == \"channels_first\":\n",
    "            batch, channels, width, height = K.int_shape(x)\n",
    "            features = K.batch_flatten(x)\n",
    "        else:\n",
    "            batch, width, height, channels = K.int_shape(x)\n",
    "            features = K.batch_flatten(K.permute_dimensions(x, (0, 3, 1, 2)))\n",
    "\n",
    "        gram = K.dot(features, K.transpose(features)) # / (channels * width * height)\n",
    "    return gram\n",
    "\n",
    "\n",
    "def l2_norm(x):\n",
    "    return K.sqrt(K.sum(K.square(x)))\n",
    "\n",
    "\n",
    "def attention_vector(x):\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        batch, channels, width, height = K.int_shape(x)\n",
    "        filters = K.batch_flatten(K.permute_dimensions(x, (1, 0, 2, 3)))  # (channels, batch*width*height)\n",
    "    else:\n",
    "        batch, width, height, channels = K.int_shape(x)\n",
    "        filters = K.batch_flatten(K.permute_dimensions(x, (3, 0, 1, 2)))  # (channels, batch*width*height)\n",
    "\n",
    "    filters = K.mean(K.square(filters), axis=0)  # (batch*width*height,)\n",
    "    filters = filters / l2_norm(filters)  # (batch*width*height,)\n",
    "    return filters\n",
    "\n",
    "\n",
    "def total_loss(y_true, y_pred):\n",
    "    mse_loss = mse_weight * mean_squared_error(y_true, y_pred)\n",
    "    perceptual_loss = perceptual_weight * K.sum(K.square(gram_matrix(y_true) - gram_matrix(y_pred)))\n",
    "    attention_loss = attention_weight * l2_norm(attention_vector(y_true) - attention_vector(y_pred))\n",
    "\n",
    "    return mse_loss + perceptual_loss + attention_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mobilenet_model(img_size, lr=1e-3):\n",
    "    '''\n",
    "    Creates a Colorizer model. Note the difference from the report\n",
    "    - https://github.com/baldassarreFe/deep-koalarization/blob/master/report.pdf\n",
    "    I use a long skip connection network to speed up convergence and\n",
    "    boost the output quality.\n",
    "    '''\n",
    "    ## Encoder Model\n",
    "    encoder_input = Input(shape=(img_size, img_size, 1,))\n",
    "    encoder1 = Conv2D(64, (3, 3), padding='same', activation='relu', strides=(2, 2))(encoder_input)\n",
    "    encoder = Conv2D(128, (3, 3), padding='same', activation='relu')(encoder1)\n",
    "    encoder2 = Conv2D(128, (3, 3), padding='same', activation='relu', strides=(2, 2))(encoder)\n",
    "    encoder = Conv2D(256, (3, 3), padding='same', activation='relu')(encoder2)\n",
    "    encoder = Conv2D(256, (3, 3), padding='same', activation='relu', strides=(2, 2))(encoder)\n",
    "    encoder = Conv2D(512, (3, 3), padding='same', activation='relu')(encoder)\n",
    "    encoder = Conv2D(512, (3, 3), padding='same', activation='relu')(encoder)\n",
    "    encoder = Conv2D(256, (3, 3), padding='same', activation='relu')(encoder)\n",
    "\n",
    "    ## Input Fusion\n",
    "    # Decide the image shape at runtime to allow prediction on\n",
    "    # any size image, even if training is on 128x128\n",
    "    batch, height, width, channels = K.int_shape(encoder)\n",
    "\n",
    "    #mobilenet_features_ip = Input(shape=(1000,))\n",
    "    #fusion = RepeatVector(height * width)(mobilenet_features_ip)\n",
    "    #fusion = Reshape((height, width, 1000))(fusion)\n",
    "    #fusion = concatenate([encoder, fusion], axis=-1)\n",
    "    fusion = Conv2D(256, (1, 1), padding='same', activation='relu')(encoder)\n",
    "\n",
    "    ## Decoder Model\n",
    "    decoder = Conv2D(128, (3, 3), padding='same', activation='relu')(fusion)\n",
    "    decoder = UpSampling2D()(decoder)\n",
    "    #decoder = Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same', activation='relu')(decoder)\n",
    "    decoder = concatenate([decoder, encoder2], axis=-1)\n",
    "    decoder = Conv2D(64, (3, 3), padding='same', activation='relu')(decoder)\n",
    "    decoder = Conv2D(64, (3, 3), padding='same', activation='relu')(decoder)\n",
    "    decoder = UpSampling2D()(decoder)\n",
    "    #decoder = Conv2DTranspose(64, (4, 4), strides=(2, 2), padding='same', activation='relu')(decoder)\n",
    "    decoder = concatenate([decoder, encoder1], axis=-1)\n",
    "    decoder = Conv2D(32, (3, 3), padding='same', activation='relu')(decoder)\n",
    "    decoder = Conv2DTranspose(3, (4, 4), strides=(2, 2), padding='same', activation='tanh')(decoder)\n",
    "    # decoder = Conv2D(2, (3, 3), padding='same', activation='tanh')(decoder)\n",
    "    # decoder = UpSampling2D((2, 2))(decoder)\n",
    "\n",
    "    model = Model([encoder_input], decoder, name='Colorizer')\n",
    "    model.compile(optimizer=Adam(lr), loss=total_loss, metrics=[y_true_max,\n",
    "                                                                y_true_min,\n",
    "                                                                y_pred_max,\n",
    "                                                                y_pred_min])\n",
    "\n",
    "    print(\"Model built and compiled\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Summary\n",
    "model = build_mobilenet_model(image_size, 1e-3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Continue training if weights are available\n",
    "if os.path.exists(weights_file_name):\n",
    "    model.load_weights(weights_file_name)\n",
    "\n",
    "# Use Batchwise TensorBoard callback\n",
    "tensorboard = TensorBoard(batch_size=batch_size)\n",
    "checkpoint = ModelCheckpoint(weights_file_name, monitor='loss', verbose=1, save_best_only=True)\n",
    "callbacks_list = [checkpoint, tensorboard]\n",
    "\n",
    "# Train Network\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Test the model against the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best weights\n",
    "from keras.models import load_model\n",
    "best_model = load_model(weights_file_name)\n",
    "\n",
    "# Test the model\n",
    "x_test, y_test = utils.prepare_input_image_batch(test_data, batch_size=batch_size)\n",
    "\n",
    "score = best_model.evaluate(x_test, y_test, batch_size, verbose=1)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show images for test data\n",
    "To be added\n",
    "# predictions = model.predict(x_test, batch_size, verbose=1)\n",
    "#postprocess_output(x_test, predictions, image_size=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "Test the model against other images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Future Work\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
